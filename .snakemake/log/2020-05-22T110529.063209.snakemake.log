Building DAG of jobs...
Using shell: /cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	aggregate
	1	all
	1	gif_hippo
	3

[Fri May 22 11:05:29 2020]
rule aggregate:
    input: output/100610_unfold_data.pkl
    output: output/unfold_data.pkl
    jobid: 3

[Fri May 22 11:05:31 2020]
Error in rule aggregate:
    jobid: 3
    output: output/unfold_data.pkl

RuleException:
CalledProcessError in line 97 of /scratch/myousif9/snakemake_hippocampal_unfolding/Snakefile:
Command 'set -euo pipefail;  /home/myousif9/venv3/bin/python /scratch/myousif9/snakemake_hippocampal_unfolding/.snakemake/scripts/tmp40j6czph.aggreg_data.py' returned non-zero exit status 1.
  File "/scratch/myousif9/snakemake_hippocampal_unfolding/Snakefile", line 97, in __rule_aggregate
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Removing output files of failed job aggregate since they might be corrupted:
output/unfold_data.pkl
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /scratch/myousif9/snakemake_hippocampal_unfolding/.snakemake/log/2020-05-22T110529.063209.snakemake.log
